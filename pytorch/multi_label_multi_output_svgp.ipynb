{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "class RBFKernel(nn.Module):\n",
    "    def __init__(self, input_dim, lengthscale_init=1.0):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Use one lengthscale per dimension (ARD - Automatic Relevance Determination)\n",
    "        self.log_lengthscale = nn.Parameter(torch.log(torch.ones(input_dim) * lengthscale_init))\n",
    "\n",
    "    def forward(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Compute RBF (Gaussian) kernel matrix between X1 and X2 using ARD lengthscales.\n",
    "        Args:\n",
    "            X1: Tensor of shape (N1, D)\n",
    "            X2: Tensor of shape (N2, D)\n",
    "        Returns:\n",
    "            Kernel matrix of shape (N1, N2)\n",
    "        \"\"\"\n",
    "        # Ensure input is 2D\n",
    "        if X1.ndimension() == 1:\n",
    "            X1 = X1.unsqueeze(0)\n",
    "        if X2.ndimension() == 1:\n",
    "            X2 = X2.unsqueeze(0)\n",
    "\n",
    "        # Scale by lengthscale (ARD: each dimension can have a different scale)\n",
    "        X1_scaled = X1 / self.log_lengthscale.exp()\n",
    "        X2_scaled = X2 / self.log_lengthscale.exp()\n",
    "\n",
    "        # Compute squared Euclidean distance\n",
    "        sqdist = torch.cdist(X1_scaled, X2_scaled, p=2).pow(2)\n",
    "\n",
    "        # Compute RBF\n",
    "        return torch.exp(-0.5 * sqdist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMCKernel(nn.Module):\n",
    "    def __init__(self, input_dim, num_outputs, num_latents, base_kernel_cls=RBFKernel):\n",
    "        \"\"\"\n",
    "        input_dim: Dimension of input x\n",
    "        num_outputs: Number of labels (T)\n",
    "        num_latents: Number of latent processes (Q)\n",
    "        base_kernel_cls: Class of base kernel, e.g., RBFKernel\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.num_latents = num_latents\n",
    "\n",
    "        # B ∈ ℝ^{T × Q}\n",
    "        self.B = nn.Parameter(torch.randn(num_outputs, num_latents))\n",
    "\n",
    "        # Each latent process has its own kernel\n",
    "        self.kernels = nn.ModuleList([\n",
    "            base_kernel_cls(input_dim) for _ in range(num_latents)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X1, X2):\n",
    "        \"\"\"\n",
    "        Returns: Tensor of shape (num_outputs, N, M)\n",
    "        \"\"\"\n",
    "        N, M = X1.size(0), X2.size(0)\n",
    "\n",
    "        # Compute all latent kernels\n",
    "        latent_kernels = []  # Will be list of (N, M)\n",
    "        for q in range(self.num_latents):\n",
    "            k_q = self.kernels[q](X1, X2)  # (N, M)\n",
    "            latent_kernels.append(k_q.unsqueeze(0))  # (1, N, M)\n",
    "\n",
    "        K_latents = torch.cat(latent_kernels, dim=0)  # (Q, N, M)\n",
    "\n",
    "        # K_output[t] = ∑_q B[t,q]^2 * K_q\n",
    "        B = self.B  # (T, Q)\n",
    "        K_output = []\n",
    "        for t in range(self.num_outputs):\n",
    "            weights = B[t]  # (Q,)\n",
    "            weighted_k = (weights.view(-1, 1, 1) * K_latents).sum(dim=0)  # (N, M)\n",
    "            K_output.append(weighted_k.unsqueeze(0))\n",
    "\n",
    "        return torch.cat(K_output, dim=0)  # (T, N, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal, Normal\n",
    "\n",
    "class MultiLabelSVGP(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels, num_inducing, num_latents=2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.num_latents = num_latents\n",
    "        self.num_inducing = num_inducing\n",
    "\n",
    "        # Inducing inputs Z ∈ ℝ^{Q×M×D}\n",
    "        self.Z = nn.Parameter(torch.randn(num_latents, num_inducing, input_dim))\n",
    "\n",
    "        # Variational parameters: mean and L (lower-triangular) for each latent process\n",
    "        self.q_mu = nn.Parameter(torch.zeros(num_latents, num_inducing))\n",
    "        self.q_log_diag = nn.Parameter(torch.zeros(num_latents, num_inducing))  # log diag for stability\n",
    "\n",
    "        # Kernel (shared across latents), coregionalization learned via B\n",
    "        self.kernel = LMCKernel(input_dim, num_labels, num_latents)\n",
    "\n",
    "    def compute_kernel_matrices(self, X):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Kxz: (Q, N, M)\n",
    "            Kzz: (Q, M, M)\n",
    "            Kxx_diag: (Q, N)\n",
    "        \"\"\"\n",
    "        Kxz = []\n",
    "        Kzz = []\n",
    "        Kxx_diag = []\n",
    "\n",
    "        for q in range(self.num_latents):\n",
    "            k_q = self.kernel.kernels[q]\n",
    "            Zq = self.Z[q]  # (M, D)\n",
    "\n",
    "            Kxz_q = k_q(X, Zq)        # (N, M)\n",
    "            Kzz_q = k_q(Zq, Zq) + 1e-6 * torch.eye(self.num_inducing)  # (M, M)\n",
    "            Kxx_q_diag = torch.diagonal(k_q(X, X))  # (N,)\n",
    "\n",
    "            Kxz.append(Kxz_q)\n",
    "            Kzz.append(Kzz_q)\n",
    "            Kxx_diag.append(Kxx_q_diag)\n",
    "\n",
    "        return torch.stack(Kxz), torch.stack(Kzz), torch.stack(Kxx_diag)\n",
    "\n",
    "    def forward(self, X, Y, full_n=None):\n",
    "        \"\"\"\n",
    "        Compute negative ELBO for multi-label classification\n",
    "        Inputs:\n",
    "            X: (N, D)\n",
    "            Y: (N, T)\n",
    "        Returns:\n",
    "            Negative ELBO loss\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        Q = self.num_latents\n",
    "        T = self.num_labels\n",
    "\n",
    "        # Compute kernel matrices\n",
    "        Kxz, Kzz, Kxx_diag = self.compute_kernel_matrices(X)  # Q×N×M, Q×M×M, Q×N\n",
    "\n",
    "        # KL Divergence: KL[q(u) || p(u)] = sum_q KL[N(μ_q, S_q) || N(0, K_zz)]\n",
    "        kl = 0\n",
    "        for q in range(Q):\n",
    "            mu_q = self.q_mu[q]  # (M,)\n",
    "            S_q_diag = self.q_log_diag[q].exp().pow(2)  # diagonal covariance (M,)\n",
    "            Kzz_q = Kzz[q]  # (M, M)\n",
    "\n",
    "            L_q = torch.diag(S_q_diag)\n",
    "            q_u = MultivariateNormal(mu_q, scale_tril=L_q)\n",
    "            p_u = MultivariateNormal(torch.zeros_like(mu_q), covariance_matrix=Kzz_q)\n",
    "            kl += torch.distributions.kl.kl_divergence(q_u, p_u)\n",
    "\n",
    "        # Predictive mean for each latent\n",
    "        f_q_means = []  # list of shape (N,) for each q\n",
    "        for q in range(Q):\n",
    "            Kxz_q = Kxz[q]  # (N, M)\n",
    "            Kzz_q = Kzz[q]  # (M, M)\n",
    "            Kzz_inv_q = torch.linalg.inv(Kzz_q)\n",
    "            mu_q = self.q_mu[q]  # (M,)\n",
    "            f_q = Kxz_q @ Kzz_inv_q @ mu_q  # (N,)\n",
    "            f_q_means.append(f_q.unsqueeze(1))  # (N, 1)\n",
    "\n",
    "        F_q = torch.cat(f_q_means, dim=1)  # (N, Q)\n",
    "\n",
    "        # Project to T labels: f_t(x) = B_{tq} f_q(x)\n",
    "        B = self.kernel.B  # (T, Q)\n",
    "        F_pred = F_q @ B.T  # (N, T)\n",
    "\n",
    "        # Binary classification loss: sigmoid cross-entropy\n",
    "        likelihood = torch.nn.BCEWithLogitsLoss()\n",
    "        recon_loss = likelihood(F_pred, Y)\n",
    "\n",
    "        scale = N / (full_n if full_n is not None else N)\n",
    "        return recon_loss * scale + kl\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the sigmoid probabilities for each label.\n",
    "\n",
    "        Inputs:\n",
    "            X: (N, D) input data\n",
    "\n",
    "        Returns:\n",
    "            probs: (N, T) probability of each label being 1\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        N = X.shape[0]\n",
    "        Q = self.num_latents\n",
    "        T = self.num_labels\n",
    "\n",
    "        # Kernel computations\n",
    "        Kxz, Kzz, _ = self.compute_kernel_matrices(X)  # Q×N×M, Q×M×M, Q×N\n",
    "\n",
    "        f_q_means = []\n",
    "        for q in range(Q):\n",
    "            Kxz_q = Kxz[q]  # (N, M)\n",
    "            Kzz_q = Kzz[q]  # (M, M)\n",
    "            Kzz_inv_q = torch.linalg.inv(Kzz_q)\n",
    "            mu_q = self.q_mu[q]  # (M,)\n",
    "            f_q = Kxz_q @ Kzz_inv_q @ mu_q  # (N,)\n",
    "            f_q_means.append(f_q.unsqueeze(1))  # (N, 1)\n",
    "\n",
    "        F_q = torch.cat(f_q_means, dim=1)  # (N, Q)\n",
    "        B = self.kernel.B  # (T, Q)\n",
    "        F_pred = F_q @ B.T  # (N, T)\n",
    "\n",
    "        probs = torch.sigmoid(F_pred)  # convert logits to probabilities\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Toy multi-label dataset\n",
    "class ToyMultiLabelDataset(Dataset):\n",
    "    def __init__(self, n_samples=200, input_dim=2, num_labels=3):\n",
    "        super().__init__()\n",
    "        self.X = torch.randn(n_samples, input_dim)  # input_dim = 2\n",
    "        self.Y = torch.zeros(n_samples, num_labels)  # num_labels = 3\n",
    "\n",
    "        # Define labels with simple nonlinear logic\n",
    "        self.Y[:, 0] = (self.X[:, 0] > 0).float()                   # Label 1: X1 > 0\n",
    "        self.Y[:, 1] = (self.X[:, 1] > 0.5).float()                 # Label 2: X2 > 0.5\n",
    "        self.Y[:, 2] = ((self.X[:, 0]**2 + self.X[:, 1]**2) > 1).float()  # Label 3: distance from origin > 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# Create dataloader\n",
    "dataset = ToyMultiLabelDataset(n_samples=300)\n",
    "\n",
    "X_all = dataset.X\n",
    "Y_all = dataset.Y\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(list(zip(X_train, Y_train)), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(list(zip(X_test, Y_test)), batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "num_outputs = 3\n",
    "num_latents = 2\n",
    "num_inducing = 30\n",
    "\n",
    "svgp_model = MultiLabelSVGP(input_dim, num_outputs, num_latents, num_inducing)\n",
    "optimizer = torch.optim.Adam(svgp_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svgp_multilabel(model, dataloader, optimizer, full_n, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        for X, Y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(X, Y, full_n)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = model.predict(X)  # [B, L]\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(Y.cpu())\n",
    "\n",
    "        Y_pred = torch.cat(all_preds).numpy()\n",
    "        Y_true = torch.cat(all_targets).numpy()\n",
    "\n",
    "        Y_pred_binary = (Y_pred >= 0.5).astype(int)\n",
    "\n",
    "        # accuracy = accuracy_score(Y_true, Y_pred_binary)\n",
    "        accuracy = np.mean((Y_true == Y_pred_binary).mean(axis=0))\n",
    "        precision = precision_score(Y_true, Y_pred_binary, average=\"macro\", zero_division=0)\n",
    "        recall = recall_score(Y_true, Y_pred_binary, average=\"macro\", zero_division=0)\n",
    "        f1 = f1_score(Y_true, Y_pred_binary, average=\"macro\", zero_division=0)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_svgp_multilabel(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for X, Y in dataloader:\n",
    "        preds = model.predict(X)  # [B, L]\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_targets.append(Y.cpu())\n",
    "\n",
    "    Y_pred = torch.cat(all_preds).numpy()\n",
    "    Y_true = torch.cat(all_targets).numpy()\n",
    "\n",
    "    # Convert probabilities to binary using threshold = 0.5\n",
    "    Y_pred_binary = (Y_pred >= 0.5).astype(int)\n",
    "\n",
    "    accuracy_score = np.mean((Y_true == Y_pred_binary).mean(axis=0))\n",
    "\n",
    "    metrics = {\n",
    "        # \"accuracy\": accuracy_score(Y_true, Y_pred_binary),\n",
    "        \"accuracy\": accuracy_score,\n",
    "        \"precision\": precision_score(Y_true, Y_pred_binary, average=\"macro\", zero_division=0),\n",
    "        \"recall\": recall_score(Y_true, Y_pred_binary, average=\"macro\", zero_division=0),\n",
    "        \"f1_score\": f1_score(Y_true, Y_pred_binary, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.4629, Accuracy: 0.4444, Precision: 0.4213, Recall: 0.4561, F1 Score: 0.4333\n",
      "Epoch 11, Loss: 0.0857, Accuracy: 0.6875, Precision: 0.9015, Recall: 0.4995, F1 Score: 0.5334\n",
      "Epoch 21, Loss: 0.0734, Accuracy: 0.8347, Precision: 0.9074, Recall: 0.7501, F1 Score: 0.8125\n",
      "Epoch 31, Loss: 0.0615, Accuracy: 0.8917, Precision: 0.9445, Recall: 0.8381, F1 Score: 0.8865\n",
      "Epoch 41, Loss: 0.0556, Accuracy: 0.9222, Precision: 0.9460, Recall: 0.9057, F1 Score: 0.9253\n",
      "Epoch 51, Loss: 0.0503, Accuracy: 0.9222, Precision: 0.9481, Recall: 0.8983, F1 Score: 0.9213\n",
      "Epoch 61, Loss: 0.0448, Accuracy: 0.9569, Precision: 0.9823, Recall: 0.9312, F1 Score: 0.9558\n",
      "Epoch 71, Loss: 0.0420, Accuracy: 0.9597, Precision: 0.9762, Recall: 0.9389, F1 Score: 0.9571\n",
      "Epoch 81, Loss: 0.0425, Accuracy: 0.9514, Precision: 0.9810, Recall: 0.9164, F1 Score: 0.9472\n",
      "Epoch 91, Loss: 0.0388, Accuracy: 0.9722, Precision: 0.9851, Recall: 0.9575, F1 Score: 0.9711\n",
      "Epoch 100, Loss: 0.0422, Accuracy: 0.9597, Precision: 0.9709, Recall: 0.9527, F1 Score: 0.9616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9333333333333335,\n",
       " 'precision': 0.9160419790104948,\n",
       " 'recall': 0.9885057471264368,\n",
       " 'f1_score': 0.9478553406223718}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_svgp_multilabel(svgp_model, train_loader, optimizer, full_n=len(X_train), epochs=100)\n",
    "\n",
    "evaluate_svgp_multilabel(svgp_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
