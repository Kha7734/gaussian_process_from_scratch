{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80024cd5",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8affbb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29898bd",
   "metadata": {},
   "source": [
    "# Kernel \n",
    "## RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81ec23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rbf_kernel(x1, x2, length_scale=1.0, variance=1.0):\n",
    "#     \"\"\"\n",
    "#     Compute the RBF kernel between two sets of points.\n",
    "    \n",
    "#     Args:\n",
    "#         x1: First set of points (N x D).\n",
    "#         x2: Second set of points (M x D).\n",
    "#         length_scale: Length scale parameter for the RBF kernel.\n",
    "    \n",
    "#     Returns:\n",
    "#         Kernel matrix (N x M).\n",
    "#     \"\"\"\n",
    "#     dists = torch.cdist(x1, x2, p=2)\n",
    "#     kernel_matrix = variance * torch.exp(-0.5 * (dists / length_scale) ** 2)\n",
    "#     return kernel_matrix\n",
    "\n",
    "class RBFKernel(nn.Module):\n",
    "    def __init__(self, length_scale=1.0, variance=1.0):\n",
    "        super().__init__()\n",
    "        self.length_scale = nn.Parameter(torch.tensor(length_scale))\n",
    "        self.variance = nn.Parameter(torch.tensor(variance))\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        dists = torch.cdist(x1, x2)\n",
    "        return self.variance * torch.exp(-0.5 * (dists / self.length_scale) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cea29e",
   "metadata": {},
   "source": [
    "## Coregionalize Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49779a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CoregionalizeKernel:\n",
    "#     def __init__(self, base_kernel, num_tasks, rank_R, scale=0.1, min_value=1e-3, max_value=5e-2, seed=42):\n",
    "#         self.base_kernel = base_kernel\n",
    "#         self.num_tasks = num_tasks\n",
    "#         self.rank_R = rank_R\n",
    "\n",
    "#         if seed is not None:\n",
    "#             torch.manual_seed(seed)\n",
    "#             np.random.seed(seed)\n",
    "#             torch.random.manual_seed(seed)\n",
    "\n",
    "#         self.W = nn.Parameter(scale * torch.randn(num_tasks, rank_R))\n",
    "#         v = (max_value - min_value) * torch.rand(num_tasks) + min_value \n",
    "#         self.v = nn.Parameter(v)\n",
    "\n",
    "#     def compute(self, X1, task1, X2, task2): \n",
    "#         \"\"\"\n",
    "#         X1: (N1, D) input points\n",
    "#         task1: (N1,) task indices for X1\n",
    "#         X2: (N2, D) input points\n",
    "#         task2: (N2,) task indices for X2\n",
    "#         \"\"\"\n",
    "#         K_input = self.base_kernel(X1, X2)\n",
    "#         B = self.W @ self.W.T + torch.diag(self.v)\n",
    "\n",
    "#         # Task correlation part \n",
    "#         B_tasks = B[task1, :][:, task2] # (n1 x n2)\n",
    "\n",
    "#         # Final kernel\n",
    "#         K = K_input * B_tasks\n",
    "#         return K\n",
    "\n",
    "class CoregionalizeKernel(nn.Module):\n",
    "    def __init__(self, base_kernel, num_tasks, rank_R):\n",
    "        super().__init__()\n",
    "        self.base_kernel = base_kernel\n",
    "        self.W = nn.Parameter(0.1 * torch.randn(num_tasks, rank_R))\n",
    "        self.v = nn.Parameter(1e-2 * torch.ones(num_tasks))\n",
    "\n",
    "    # def forward(self, X1, task1, X2, task2):\n",
    "    #     K_input = self.base_kernel(X1, X2)\n",
    "    #     B = self.W @ self.W.T + torch.diag(self.v)\n",
    "    #     B_tasks = B[task1, :][:, task2]\n",
    "    #     return K_input * B_tasks\n",
    "\n",
    "    def forward(self, X1, task1, X2, task2):\n",
    "        task1 = task1.to(torch.long)\n",
    "        task2 = task2.to(torch.long)\n",
    "        \n",
    "        K_input = self.base_kernel(X1, X2)\n",
    "        B = self.W @ self.W.T + torch.diag(self.v)\n",
    "        B_tasks = B[task1, :][:, task2]\n",
    "        return K_input * B_tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57614624",
   "metadata": {},
   "source": [
    "# Multi-output GP Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a3daa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiOutputGPClassifier(nn.Module):\n",
    "#     def __init__(self, kernel, noise=1e-6, max_iter=20, device=None):\n",
    "#         \"\"\"\n",
    "#         coregionalization_kernel: an instance with a `.compute(X1, task1, X2, task2)` method returning a torch.Tensor\n",
    "#         noise: float, jitter for numerical stability\n",
    "#         max_iter: int, Laplace iterations\n",
    "#         device: 'cuda' or 'cpu'\n",
    "#         \"\"\"\n",
    "#         self.kernel = kernel\n",
    "#         self.noise = noise\n",
    "#         self.max_iter = max_iter\n",
    "#         self.is_trained = False\n",
    "#         self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     def fit(self, X_train, task_train, y_train):\n",
    "#         \"\"\"\n",
    "#         X_train: (n x d) torch.Tensor\n",
    "#         task_train: (n,) torch.LongTensor\n",
    "#         y_train: (n,) torch.FloatTensor (binary: 0 or 1)\n",
    "#         \"\"\"\n",
    "#         X_train = X_train.to(self.device)\n",
    "#         task_train = task_train.to(self.device)\n",
    "#         y_train = y_train.to(self.device)\n",
    "\n",
    "#         n = X_train.shape[0]\n",
    "\n",
    "#         # Compute kernel matrix\n",
    "#         K = self.kernel.compute(X_train, task_train, X_train, task_train)  # (n x n)\n",
    "#         K += self.noise * torch.eye(n, device=self.device)\n",
    "\n",
    "#         # Initialize latent function\n",
    "#         f = torch.zeros(n, device=self.device)\n",
    "\n",
    "#         for iter in range(self.max_iter):\n",
    "#             pi = torch.sigmoid(f)\n",
    "#             W_diag = pi * (1 - pi)\n",
    "#             sqrt_W = torch.sqrt(W_diag + 1e-9)\n",
    "\n",
    "#             B = torch.eye(n, device=self.device) + sqrt_W[:, None] * K * sqrt_W[None, :]\n",
    "#             B += 1e-5 * torch.eye(B.shape[0], device=self.device)\n",
    "#             L = torch.linalg.cholesky(B)\n",
    "\n",
    "#             b = W_diag * f + (y_train - pi)\n",
    "#             # Solve: a = b - sqrt_W * solve(L, solve(L.T, sqrt_W * (K @ b)))\n",
    "#             temp = torch.cholesky_solve((sqrt_W[:, None] * (K @ b).unsqueeze(1)), L)\n",
    "#             a = b - sqrt_W * temp.squeeze(1)\n",
    "#             f = K @ a\n",
    "\n",
    "#             if iter % 5 == 0:\n",
    "#                 ll = torch.sum(y_train * torch.log(pi + 1e-6) + (1 - y_train) * torch.log(1 - pi + 1e-6))\n",
    "#                 print(f\"Iteration {iter}: Bernoulli Log-Likelihood = {ll.item():.4f}\")\n",
    "\n",
    "#         self.f_hat = f\n",
    "#         self.X_train = X_train\n",
    "#         self.task_train = task_train\n",
    "#         self.W_diag = W_diag\n",
    "#         self.L = L\n",
    "#         self.y_train = y_train\n",
    "#         self.is_trained = True\n",
    "\n",
    "#     def compute_nll(self, X, task_ids, y):\n",
    "#         X = X.to(self.device)\n",
    "#         task_ids = task_ids.to(self.device)\n",
    "#         y = y.to(self.device)\n",
    "\n",
    "#         n = X.shape[0]\n",
    "#         K = self.kernel(X, task_ids, X, task_ids)\n",
    "#         K += self.noise * torch.eye(n, device=self.device)\n",
    "\n",
    "#         f = torch.zeros(n, device=self.device)  # Initial latent function\n",
    "\n",
    "#         for _ in range(self.max_iter):\n",
    "#             pi = torch.sigmoid(f)\n",
    "#             W_diag = pi * (1 - pi)\n",
    "#             sqrt_W = torch.sqrt(W_diag + 1e-9)\n",
    "\n",
    "#             B = torch.eye(n, device=self.device) + sqrt_W[:, None] * K * sqrt_W[None, :]\n",
    "#             L = torch.linalg.cholesky(B)\n",
    "\n",
    "#             b = W_diag * f + (y - pi)\n",
    "#             temp = torch.cholesky_solve((sqrt_W[:, None] * (K @ b).unsqueeze(1)), L)\n",
    "#             a = b - sqrt_W * temp.squeeze(1)\n",
    "#             f = K @ a\n",
    "\n",
    "#         # Save variables for prediction\n",
    "#         self.f_hat = f.detach()\n",
    "#         self.X_train = X.detach()\n",
    "#         self.task_train = task_ids.detach()\n",
    "#         self.W_diag = W_diag.detach()\n",
    "#         self.L = L.detach()\n",
    "#         self.y_train = y.detach()\n",
    "\n",
    "#         # Negative Log Likelihood\n",
    "#         pi = torch.sigmoid(f)\n",
    "#         nll = -torch.sum(y * torch.log(pi + 1e-6) + (1 - y) * torch.log(1 - pi + 1e-6))\n",
    "\n",
    "#         return nll\n",
    "\n",
    "#     def predict(self, X_test, task_test):\n",
    "#         if not self.is_trained:\n",
    "#             raise RuntimeError(\"Train the model first!\")\n",
    "\n",
    "#         X_test = X_test.to(self.device)\n",
    "#         task_test = task_test.to(self.device)\n",
    "\n",
    "#         K_s = self.kernel.compute(self.X_train, self.task_train, X_test, task_test)  # (n_train x n_test)\n",
    "\n",
    "#         f_mean = K_s.T @ (self.y_train - torch.sigmoid(self.f_hat))  # (n_test,)\n",
    "\n",
    "#         sqrt_W = torch.sqrt(self.W_diag + 1e-9)\n",
    "#         v = torch.cholesky_solve((sqrt_W[:, None] * K_s), self.L)\n",
    "#         K_test = self.kernel.compute(X_test, task_test, X_test, task_test)\n",
    "#         f_var = torch.diagonal(K_test) - torch.sum(v**2, dim=0)\n",
    "#         f_var = torch.clamp(f_var, min=1e-6)\n",
    "\n",
    "#         gamma = 1.0 / torch.sqrt(1.0 + (np.pi * f_var) / 8.0)\n",
    "#         probs = torch.sigmoid(gamma * f_mean)\n",
    "\n",
    "#         return probs.cpu()  # move back to CPU if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abce87cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputGPClassifier(nn.Module):\n",
    "    def __init__(self, kernel, noise=1e-6, max_iter=20, device=None):\n",
    "        super().__init__()\n",
    "        self.kernel = kernel\n",
    "        self.noise = noise\n",
    "        self.max_iter = max_iter\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def compute_nll(self, X, task_ids, y):\n",
    "        X = X.to(self.device)\n",
    "        task_ids = task_ids.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        n = X.shape[0]\n",
    "        K = self.kernel(X, task_ids, X, task_ids)\n",
    "        K += self.noise * torch.eye(n, device=self.device)\n",
    "\n",
    "        f = torch.zeros(n, device=self.device)  # Initial latent function\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            pi = torch.sigmoid(f)\n",
    "            W_diag = pi * (1 - pi)\n",
    "            sqrt_W = torch.sqrt(W_diag + 1e-9)\n",
    "\n",
    "            B = torch.eye(n, device=self.device) + sqrt_W[:, None] * K * sqrt_W[None, :]\n",
    "            L = torch.linalg.cholesky(B)\n",
    "\n",
    "            b = W_diag * f + (y - pi)\n",
    "            temp = torch.cholesky_solve((sqrt_W[:, None] * (K @ b).unsqueeze(1)), L)\n",
    "            a = b - sqrt_W * temp.squeeze(1)\n",
    "            f = K @ a\n",
    "\n",
    "        # Save variables for prediction\n",
    "        self.f_hat = f.detach()\n",
    "        self.X_train = X.detach()\n",
    "        self.task_train = task_ids.detach()\n",
    "        self.W_diag = W_diag.detach()\n",
    "        self.L = L.detach()\n",
    "        self.y_train = y.detach()\n",
    "\n",
    "        # Negative Log Likelihood\n",
    "        pi = torch.sigmoid(f)\n",
    "        nll = -torch.sum(y * torch.log(pi + 1e-6) + (1 - y) * torch.log(1 - pi + 1e-6))\n",
    "\n",
    "        return nll\n",
    "\n",
    "    def predict(self, X_test, task_ids_test):\n",
    "        self.eval()\n",
    "        X_test = X_test.to(self.device)\n",
    "        task_ids_test = task_ids_test.to(self.device)\n",
    "\n",
    "        # Compute cross-covariance between test and train\n",
    "        K_star = self.kernel(X_test, task_ids_test, self.X_train, self.task_train)\n",
    "\n",
    "        # Recompute pi and W_diag to get 'a' again\n",
    "        pi = torch.sigmoid(self.f_hat)\n",
    "        W_diag = self.W_diag\n",
    "        b = W_diag * self.f_hat + (self.y_train - pi)\n",
    "\n",
    "        # Solve for 'a'\n",
    "        sqrt_W = torch.sqrt(W_diag + 1e-9)\n",
    "        temp = torch.cholesky_solve((sqrt_W[:, None] * (self.kernel(self.X_train, self.task_train, self.X_train, self.task_train) @ b).unsqueeze(1)), self.L)\n",
    "        a = b - sqrt_W * temp.squeeze(1)\n",
    "\n",
    "        # Predictive mean\n",
    "        f_mean = K_star @ a\n",
    "\n",
    "        # Predict probability via sigmoid\n",
    "        prob = torch.sigmoid(f_mean)\n",
    "\n",
    "        return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91415374",
   "metadata": {},
   "source": [
    "# Dataset Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89f2d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generation(n_samples=100, n_tasks=3, seed=None):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for multi-output GP classification.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate.\n",
    "        n_tasks: Number of tasks (outputs).\n",
    "        seed: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        X: Input features (n_samples x 2).\n",
    "        y: Target labels (n_samples,).\n",
    "        task_train: Task indices for training data (n_samples,).\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    X = torch.randn(n_samples, 2)\n",
    "    # y = torch.randint(0, 2, (n_samples,))\n",
    "    # y = (X[:, 0] * X[:, 1] > 0).float()\n",
    "    task_train = torch.randint(0, n_tasks, (n_samples,))\n",
    "    \n",
    "    y = torch.where(task_train == 0, (X[:, 0] > 0), \n",
    "    torch.where(task_train == 1, (X[:, 1] > 0), \n",
    "                (X[:, 0] + X[:, 1] > 0))).float()\n",
    "\n",
    "    return X, y, task_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec97bba",
   "metadata": {},
   "source": [
    "# Main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d76295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xs/51r1s2s93p3c9myn9mtzl2gc0000gn/T/ipykernel_35263/2446697243.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n",
      "/var/folders/xs/51r1s2s93p3c9myn9mtzl2gc0000gn/T/ipykernel_35263/2446697243.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.float32)\n",
      "/var/folders/xs/51r1s2s93p3c9myn9mtzl2gc0000gn/T/ipykernel_35263/2446697243.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  task_train = torch.tensor(task_train, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "num_tasks = 2\n",
    "\n",
    "model = MultiOutputGPClassifier(\n",
    "    kernel=CoregionalizeKernel(\n",
    "        base_kernel=RBFKernel(length_scale=1.0, variance=1.0),\n",
    "        num_tasks=num_tasks,\n",
    "        rank_R=2\n",
    "        # scale=0.1,\n",
    "        # min_value=1e-3,\n",
    "        # max_value=5e-2,\n",
    "        # seed=42\n",
    "    ),\n",
    "    max_iter=20,\n",
    "    noise=1e-5\n",
    ")\n",
    "\n",
    "\n",
    "# 1. Generate Data\n",
    "X, y, task_train = dataset_generation(n_samples=300, n_tasks=num_tasks, seed=42)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "task_train = torch.tensor(task_train, dtype=torch.long)\n",
    "\n",
    "# 2. Train/Test Split\n",
    "X_train, X_test, y_train, y_test, task_train_split, task_test_split = train_test_split(\n",
    "    X, y, task_train, test_size=0.2, random_state=42, stratify=task_train\n",
    ")\n",
    "\n",
    "# 3. Train Model\n",
    "# model.fit(X_train, task_train_split, y_train)\n",
    "\n",
    "# 4. Predict\n",
    "# probs = model.predict(X_test, task_test_split)\n",
    "# preds = (probs > 0.5).float()\n",
    "\n",
    "# # 5. Evaluate\n",
    "# acc = accuracy_score(y_test.detach().numpy(), preds.detach().numpy())\n",
    "# auc = roc_auc_score(y_test.detach().numpy(), probs.detach().numpy())\n",
    "\n",
    "# print(f\"Test Accuracy: {acc:.4f}\")\n",
    "# print(f\"Test ROC-AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 149.9748\n",
      "Epoch 10 - Loss: 99.6918\n",
      "Epoch 20 - Loss: 77.5921\n",
      "Epoch 30 - Loss: 65.0688\n",
      "Epoch 40 - Loss: 57.0103\n",
      "Epoch 50 - Loss: 51.3285\n",
      "Epoch 60 - Loss: 47.0646\n",
      "Epoch 70 - Loss: 43.7065\n",
      "Epoch 80 - Loss: 40.9669\n",
      "Epoch 90 - Loss: 38.6716\n",
      "Epoch 100 - Loss: 36.7086\n",
      "Epoch 110 - Loss: 35.0025\n",
      "Epoch 120 - Loss: 33.5000\n",
      "Epoch 130 - Loss: 32.1624\n",
      "Epoch 140 - Loss: 30.9603\n",
      "Epoch 150 - Loss: 29.8713\n",
      "Epoch 160 - Loss: 28.8779\n",
      "Epoch 170 - Loss: 27.9660\n",
      "Epoch 180 - Loss: 27.1243\n",
      "Epoch 190 - Loss: 26.3435\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, X, task_ids, y, lr=0.01, epochs=100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.compute_nll(X, task_ids, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "trained_model = train_model(model, X_train, task_train_split, y_train, lr=0.01, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf6a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Evaluation Metrics:\n",
      "  🔹 Accuracy:  0.9833\n",
      "  🔹 Precision: 1.0000\n",
      "  🔹 Recall:    0.9600\n",
      "  🔹 F1-score:  0.9796\n",
      "\n",
      "🧮 Confusion Matrix:\n",
      "[[35  0]\n",
      " [ 1 24]]\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, X_test, task_ids_test, y_test, threshold=0.5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predicted probabilities\n",
    "    with torch.no_grad():\n",
    "        probs = model.predict(X_test, task_ids_test)\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    y_pred = (probs >= threshold).long()\n",
    "\n",
    "    # Convert to NumPy\n",
    "    y_true_np = y_test.cpu().numpy()\n",
    "    y_pred_np = y_pred.cpu().numpy()\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_true_np, y_pred_np)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_np, y_pred_np, average='binary')\n",
    "    cm = confusion_matrix(y_true_np, y_pred_np)\n",
    "\n",
    "    print(\"📊 Evaluation Metrics:\")\n",
    "    print(f\"  🔹 Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  🔹 Precision: {precision:.4f}\")\n",
    "    print(f\"  🔹 Recall:    {recall:.4f}\")\n",
    "    print(f\"  🔹 F1-score:  {f1:.4f}\")\n",
    "    print(\"\\n🧮 Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n",
    "    \n",
    "# Evaluate the trained model\n",
    "evaluation = evaluate_model(trained_model, X_test, task_test_split, y_test, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38130031",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a251c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_output_gpc(model, X_train, task_train, y_train, X_test, task_test, y_test):\n",
    "    \"\"\"\n",
    "    Visualize the predictions of the multi-output GP classifier.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained multi-output GP classifier.\n",
    "        X_train: Training input features.\n",
    "        task_train: Task indices for training data.\n",
    "        y_train: Training target labels.\n",
    "        X_test: Test input features.\n",
    "        task_test: Task indices for test data.\n",
    "        y_test: Test target labels.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = model.predict(X_test, task_test)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(model.kernel.num_tasks):\n",
    "        plt.subplot(1, model.kernel.num_tasks, i + 1)\n",
    "        plt.scatter(X_train[task_train == i][:, 0], X_train[task_train == i][:, 1], c=y_train[task_train == i], cmap='coolwarm', label='Train')\n",
    "        plt.scatter(X_test[task_test == i][:, 0], X_test[task_test == i][:, 1], c=probs[task_test == i], cmap='coolwarm', marker='x', label='Test Predicted')\n",
    "        plt.title(f'Task {i + 1}')\n",
    "        plt.colorbar()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# plot_multi_output_gpc(model, X_train, task_train_split, y_train, X_test, task_test_split, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3d4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e921c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b749af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787efd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03175e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
